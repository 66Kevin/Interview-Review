[TOC]

# Week2 Embedded Agents

Agents are embedded in an environment, meaning that an agent affects and is affected by the environment. An agent experiences its environment through sensors an acts on its environment through effectors. 

## 2.1 Math revision

- Power Set: The power set of a set A is the set of all subsets of A. It is written as $2^A$
  - $A=\{xâˆ£0<x<7$ and $x$ is odd$\}$. A={1,3,5} hence $2^A$={âˆ…,{1},{3},{5},{1,3},{1,5},{3,5},{1,3,5}}

## 2.2 Accessible and inaccessible environments

- An accessible environment is one in which the agent can obtain complete, accurate, up-to-date information about the environmentâ€™s state

- Most moderately complex environments (including, for example, the everyday physical world and the Internet) are inaccessible.

Example:

- Accessible: Chess
- Inacessible: Stock market

## 2.3 Deterministic and non-deterministic environments

- A deterministic environment is one in which every action has a single guaranteed effect â€” there is no uncertainty about the state that will result from performing an action.
- The physical world can to all intents and purposes be regarded as non-deterministic.

## 2.4 Static and dynamic environments

- A static environment is one in which the only changes to the environment are those caused by actions made by the agent.å¯¹ç¯å¢ƒçš„å”¯ä¸€æ›´æ”¹æ˜¯ç”±agentæ‰§è¡Œçš„æ“ä½œå¼•èµ·çš„æ›´æ”¹ã€‚

- A dynamic environment is one that has other processes and/or agents operating on it, and so changes in ways beyond the agentâ€™s control. ä¸ä»…ä»…æ˜¯æœ‰ä¸€ä¸ªagentåœ¨å†³ç­–ï¼Œå…¶ä»–agentsåœ¨å†³ç­–æ—¶ä¼šäº’ç›¸å½±å“

## 2.5 Formal specification of an embedded agent

$Env = <E, e_0, \pi>$

## 2.6 Utility Functions

Utility functions allow an agent to understand how "good" an outcome is. 



# Week3:Deductive,reactive and hybrid reasoning agents

- Deductive reasoning agents, which have an explicitly model of the world represented in logical formulas and reason using logical deduction. 
  - Concurrent MetateM, a specific language for specifying deductive reasoning agents.

- Reactive agents, which do not have an explicit model of the world that they reason with, but instead their behaviour is driven by a set of "stimulus -> action" rules.
  - The subsumption architecture, probably the most well known reactive agent architecture.

- Hybrid agents, which combine a purely reactive layer with higher-level reasoning layers.
  - We'll look at TouringMachines and InteRRaP as two examples of these. 


## 3.1 Deductive Agents

### 3.1.1 Concurrent MetateM

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211225184017386.png" alt="image-20211225184017386" style="zoom:40%;" />

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211225184524543.png" alt="image-20211225184524543" style="zoom:40%;" />

E.g:

$student(you)$  $\upsilon$ $graduate(you)$ -- at some point in the future you will graduate, until then you will be a student. ç›´åˆ°æ¯•ä¸šå‰ä½ ä¸€ç›´æ˜¯å­¦ç”Ÿ

$$title(me,dr)$$ $S$ ğ‘ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘’ğ‘‘(ğ‘šğ‘’,ğ‘â„ğ‘‘) -- at some point in the past I was awarded a PhD and my title has been Dr ever since then. ä»æˆäºˆæˆ‘PHDå­¦ä½ä»¥åï¼Œæˆ‘ä¸€ç›´éƒ½æ˜¯Dr

$Â¬ğ‘“ğ‘Ÿğ‘–ğ‘’ğ‘›ğ‘‘ğ‘ (ğ‘šğ‘’,ğ‘¦ğ‘œğ‘¢)$ $W $ $ğ‘ğ‘ğ‘œğ‘™ğ‘œğ‘”ğ‘–ğ‘ ğ‘’(ğ‘¦ğ‘œğ‘¢,ğ‘šğ‘’)$ -- we will never be friends unless you apologise to me.

$title(you,dr)$ $Z$ $awarded(you,phd)$ -- if it's true that you've been awarded a PhD in the past, then your title has been Dr ever since then.

## 3.2 Reactive Agents

 ä¸å†ä½¿ç”¨åƒMetaMä¸­é‚£æ ·çš„ç¬¦å·æ¥è¡¨ç¤ºè¡Œä¸ºï¼Œåªæ˜¯é€šè¿‡å“åº”ç¯å¢ƒè€Œäº§ç”Ÿçš„å„ç§ç®€å•è¡Œä¸ºçš„ç›¸äº’ä½œç”¨äº§ç”Ÿæ™ºèƒ½è¡Œä¸ºã€‚

### 3.2.1 subsumption architecture

subsumption architecture - the most famous reactive agent architecture

æŠŠè¡Œä¸ºç»„ç»‡æˆç­‰çº§å±‚æ¬¡ç»“æ„ï¼Œç­‰çº§å±‚çº§ç»“æ„ä¸­çš„ä½å±‚å¯ä»¥æŠ‘åˆ¶é«˜å±‚ï¼Œå±‚æ¬¡è¶Šä½ä¼˜å…ˆåº¦è¶Šé«˜ï¼Œ**å…¶ä¸»è¦æ€æƒ³å°±æ˜¯å±‚æ¬¡è¶Šé«˜è¡¨ç¤ºçš„è¶ŠæŠ½è±¡çš„è¡Œä¸º**ä¸¾ä¾‹æ¥è¯´ï¼Œæœ‰äººå¸Œæœ›ç§»åŠ¨æœºå™¨ï¼Œäººæœ‰èº²é¿éšœç¢çš„è¡Œä¸ºã€‚è¿™æ˜¾ç„¶è¦ç»™èº²é¿éšœç¢ç‰©èµ‹äºˆä¸€ä¸ªé«˜çš„ä¼˜å…ˆçº§ã€‚

ä¸¾ä¾‹ï¼šç«æ˜Ÿè½¦Steels' experiments with the Mars explorer system

å±€é™æ€§ï¼š

- ç”±äºçº¯ååº”å¼ Agent æŒ‰ç…§å±€éƒ¨ä¿¡æ¯ï¼ˆå³å…³äºAgentå½“å‰çŠ¶æ€çš„ä¿¡æ¯ï¼‰åšå‡ºå¤®ç­–ï¼Œå¾ˆéš¾æƒ³åƒè¿™ç§å†³ç­–æ–¹æ³•èƒ½è€ƒè™‘éå±€éƒ¨ä¿¡æ¯
- å¿…é¡»ç»è¿‡è´¹åŠ›è´¹æ—¶æ‰èƒ½åˆ›é€ å‡ºçº¯ååº”å¼ Agent
- Agentså¯ç”¨çš„è¡Œä¸ºè¶Šå¤šï¼Œç†è§£æˆ–é¢„æµ‹å®ƒä»¬çš„è¡Œä¸ºå°±è¶Šå›°éš¾ï¼Œå› ä¸ºä¸åŒå±‚ä¹‹é—´çš„ç›¸äº’ä½œç”¨çš„åŠ¨æ€å˜å¾—å¤ªå¤æ‚è€Œæ— æ³•ç†è§£ã€‚

## 3.3 Hybrid Agents



## Week2: Practical Reasoning Agents

## 2.1 Delibration and Means-ends reasoning

- **Deliberation**: deciding what state of affairs the agent wants to achieve; the results of deliberation are referred to as the agent's **intentions**. ( **weighing up** the options available to me and **deciding** which ones to commit to as intentions.)

- **Means-ends reasoning**: deciding how to achieve these states of affairs. (Once I've decided on my intention, I need to work out what I'm going to do to try to achieve this.)
- **Intentions**: We have seen that the output of the agent's deliberation are **intentions**. If an agent has an intention X this means is intends to (try) to bring about the state of affairs X.

## 2.2 Resource bounded reasoning and calculative rationality

calculative rationality

E.g:

Is the following statement true or false?

An agent with the property of calculative rationality that is situated in a static environment is guaranteed to always make optimal decisions.

TRUE

If an agent has the property of calculative rationality, the decisions it makes are guaranteed to be optimal according to the information it had available when it started making its decision. If the environment is static, this means that the only way the world can change is if the agent itself performs some action. This means that the world is guaranteed not to change during the time the agent is deciding what to do, and so a decision that is optimal according to the information available when the decision making process started will still be optimal when the agent finishes making its decision.

## 2.3 BDI agents

BDI stands for **Beliefs, Desires, Intentions**

Based on an agent's beliefs, it has a set of desires. Once an agent commits to trying to achieve one of its desires, this desire becomes an intention. 

E.g: If my workload eases then I might decide it is feasible to visit my family and commit to this as an intention. Once I have committed to this as an intention, we expect (as discussed previously) that: I will invest some effort in trying to achieve this;

### 2.3.1 BDI V1

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226025455604.png" alt="image-20211226025455604" style="zoom:40%;" />

- When an agent receives a new percept from the environment, it updates its beliefs to take this new information into account using its belief revision function: $brf()$.
- $options()$, which takes as input the agent's current beliefs and intentions and returns a set of desires;
- $filter()$ function, which takes as input the agent's current beliefs, desires and intentions and returns the "best" options to commit to as its new intentions.
- $plan()$ Function,  which takes the agent's current beliefs and intentions, and returns a plan for the agent to follow in order to try to achieve its intentions. 
- Means-ends reasoning is concerned with working out what the agent is going to do to try to achieve its intentions. This is what the $plan()$ function does: determines a plan the agent is going to employ to try to achieve its intentions.
- **Deliberation is a two step process**. First the agent identifies its options (line 6) and then the agent filters these to select its intentions (line 7). Deliberation therefore takes place over lines 6 and 7.

### 2.3.2 BDI V2

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226030123046.png" alt="image-20211226030123046" style="zoom:40%;" />

Version 2 of our agent control loop allows the agent to execute one action from its plan at a time, pausing to observe the environment after each action and reconsider whether its plan is still appropriate based on its new beliefs.

- $empty(Ï€)$ is true if and only if the plan Ï€ is empty.
- $â„ğ‘’ğ‘ğ‘‘(ğœ‹)$ returns the first action of the plan Ï€.
- $ğ‘¡ğ‘ğ‘–ğ‘™(ğœ‹)$ returns the remainder of the plan Ï€ once the first action of ğœ‹ has been removed.
- $ğ‘ ğ‘œğ‘¢ğ‘›ğ‘‘(ğœ‹,ğ¼,ğµ)$ is true if and only if, according to the agent's beliefs ğµ, the agent can expect the plan ğœ‹ to achieve its intentions ğ¼. 



### 2.3.3 BDI V3

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226030927828.png" alt="image-20211226030927828" style="zoom:40%;" />

In version 2 of the control loop, the only time the agent ever stops to consider its intentions is when it has completed executing its plan. We will now adapt the control loop so that the agent is able to adjust its intentions if it is appropriate to do so. For this we need to introduce the following predicates.

- $succeeded(I,B)$ is true if and only if, based on its beliefs B, the agent believes it has achieved its intentions ğ¼.
- $impossible(I,B)$ is true if and only if, based on its beliefs B, the agent believes it is impossible to achieve its intentions ğ¼

If the agent believes that its intentions are either impossible or already achieved, there is no point in the agent persisting in trying to achieve those intentions. 

Agents using version 3 of the BDI control loop stop and deliberate again in order to reconsider their intentions after every action they execute (lines 15-16 of control loop version 3).



### 2.3.4 BDI V4 (Meta-level)

æ…æ€çš„è¿‡ç¨‹éœ€è¦èŠ±è´¹å¤§é‡çš„æ—¶é—´ï¼Œå½“ä¸€ä¸ªagentåœ¨æ…æ€æ—¶ï¼Œå‘¨å›´çš„ç¯å¢ƒä¹Ÿåœ¨æ”¹å˜ï¼Œå¯èƒ½ä¼šæå‡ºæ²¡æœ‰å…³ç³»çš„æ–°å½¢æˆçš„æ„å›¾

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226031704202.png" alt="image-20211226031704202" style="zoom:40%;" />



-  $reconsider(I,B)$ is true if and only if, given its beliefs B, the agent believes it is appropriate to reconsider its intentions ğ¼.

There is an important implicit assumption here: the cost of determining whether **reconsider(I,B) is true must be less than the cost of the actual deliberation** process itself (i.e., performing functions options and filters). If this were not the case, then the agent might as well go ahead and do the deliberation, since this would be less costly than trying to decide whether it should deliberate or not.



E.gï¼š

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226033608068.png" alt="image-20211226033608068" style="zoom:50%;" />

agentsæ…æ€ä¹‹åå°±ä¼šæ”¹å˜intentionï¼Œå› ä¸ºdeliberationæ˜¯ä¸¤éƒ¨åˆ†ç»„æˆï¼ˆ **weighing up** the options available to me and **deciding** which ones to commit to as intentionsï¼‰ã€‚å½“ä¸”ä»…å½“Agenté€‰æ‹©é•‡æ€çš„æ—¶å€™ï¼Œä½¿Agent æ”¹å˜æ„å›¾ï¼Œä¸¤æ•°reconsider (â€¦)çš„è¡Œä¸ºæ˜¯æœ€ä¼˜çš„ã€‚å¯¹äº
Agent é€‰æ‹©äº†æ…æ€è¿‡ç¨‹ï¼Œä½†æ²¡æœ‰æ”¹äº¤æ„å›¾æ—¶ï¼Œåˆ™åœ¨æ…æ€è¿‡ç¨‹ä¸­èŠ±è´¹çš„åŠªåŠ›å°±æ˜¯æµªè´¹ã€‚åŒæ ·ï¼Œå¦‚æœ Agentåº”è¯¥æ”¹å˜æ„å›¾ï¼Œä½†æ˜¯æ²¡æœ‰èƒ½æ”¹å˜å¤±è´¥äº†ï¼Œåˆ™èŠ±è´¹åœ¨å®ç°æ„å›¾ä¸Šçš„åŠªåŠ›ä¹Ÿæ˜¯æµªè´¹çš„ã€‚å› æ­¤1å’Œ4æœ€ä¼˜
